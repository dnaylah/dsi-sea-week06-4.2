{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Lab\n",
    "\n",
    "In this lab we will further explore Scikit's and NLTK's capabilities to process text. We will use the 20 Newsgroup dataset, which is provided by Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"sklearn.datasets.twenty_newsgroups\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data inspection\n",
    "\n",
    "We have downloaded a few newsgroup categories and removed headers, footers and quotes.\n",
    "\n",
    "Let's inspect them.\n",
    "\n",
    "1. What data taype is `data_train`\n",
    "> sklearn.datasets.base.Bunch\n",
    "- Is it like a list? Or like a Dictionary? or what?\n",
    "> Dict\n",
    "- How many data points does it contain?\n",
    "- Inspect the first data point, what does it look like?\n",
    "> A blurb of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u\"Hi,\\n\\nI've noticed that if you only save a model (with all your mapping planes\\npositioned carefully) to a .3DS file that when you reload it after restarting\\n3DS, they are given a default position and orientation.  But if you save\\nto a .PRJ file their positions/orientation are preserved.  Does anyone\\nknow why this information is not stored in the .3DS file?  Nothing is\\nexplicitly said in the manual about saving texture rules in the .PRJ file. \\nI'd like to be able to read the texture rule information, does anyone have \\nthe format for the .PRJ file?\\n\\nIs the .CEL file format available from somewhere?\\n\\nRych\", u'\\n\\nSeems to be, barring evidence to the contrary, that Koresh was simply\\nanother deranged fanatic who thought it neccessary to take a whole bunch of\\nfolks with him, children and all, to satisfy his delusional mania. Jim\\nJones, circa 1993.\\n\\n\\nNope - fruitcakes like Koresh have been demonstrating such evil corruption\\nfor centuries.', u\"\\n >In article <1993Apr19.020359.26996@sq.sq.com>, msb@sq.sq.com (Mark Brader) \\n\\nMB>                                                             So the\\nMB> 1970 figure seems unlikely to actually be anything but a perijove.\\n\\nJG>Sorry, _perijoves_...I'm not used to talking this language.\\n\\nCouldn't we just say periapsis or apoapsis?\\n\\n \", u'I have a request for those who would like to see Charley Wingate\\nrespond to the \"Charley Challenges\" (and judging from my e-mail, there\\nappear to be quite a few of you.)  \\n\\nIt is clear that Mr. Wingate intends to continue to post tangential or\\nunrelated articles while ingoring the Challenges themselves.  Between\\nthe last two re-postings of the Challenges, I noted perhaps a dozen or\\nmore posts by Mr. Wingate, none of which answered a single Challenge.  \\n\\nIt seems unmistakable to me that Mr. Wingate hopes that the questions\\nwill just go away, and he is doing his level best to change the\\nsubject.  Given that this seems a rather common net.theist tactic, I\\nwould like to suggest that we impress upon him our desire for answers,\\nin the following manner:\\n\\n1. Ignore any future articles by Mr. Wingate that do not address the\\nChallenges, until he answers them or explictly announces that he\\nrefuses to do so.\\n\\n--or--\\n\\n2. If you must respond to one of his articles, include within it\\nsomething similar to the following:\\n\\n    \"Please answer the questions posed to you in the Charley Challenges.\"\\n\\nReally, I\\'m not looking to humiliate anyone here, I just want some\\nhonest answers.  You wouldn\\'t think that honesty would be too much to\\nask from a devout Christian, would you?  \\n\\nNevermind, that was a rhetorical question.', u'AW&ST  had a brief blurb on a Manned Lunar Exploration confernce\\nMay 7th  at Crystal City Virginia, under the auspices of AIAA.\\n\\nDoes anyone know more about this?  How much, to attend????\\n\\nAnyone want to go?']\n"
     ]
    }
   ],
   "source": [
    "print data_train.data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bag of Words model\n",
    "\n",
    "Let's train a model using a simple count vectorizer\n",
    "\n",
    "1. Initialize a standard CountVectorizer and fit the training data\n",
    "- how big is the feature dictionary\n",
    "- repeat eliminating english stop words\n",
    "- is the dictionary smaller?\n",
    "- transform the training data using the trained vectorizer\n",
    "- what are the 20 words that are most common in the whole corpus?\n",
    "- what are the 20 most common words in each of the 4 classes?\n",
    "- evaluate the performance of a Lotistic Regression on the features extracted by the CountVectorizer\n",
    "    - you will have to transform the test_set too. Be carefule to use the trained vectorizer, without re-fitting it\n",
    "- try the following 3 modification:\n",
    "    - restrict the max_features\n",
    "    - change max_df and min_df\n",
    "    - use a fixed vocabulary of size 80 combining the 20 most common words per group found earlier\n",
    "- for each of the above print a confusion matrix and investigate what gets mixed\n",
    "> Anwer: not surprisingly if we reduce the feature space we lose accuracy\n",
    "- print out the number of features for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = CountVectorizer()\n",
    "x_train = v.fit_transform(data_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 26879)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 26576)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = CountVectorizer(stop_words=\"english\")\n",
    "x_train = v.fit_transform(data_train.data)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is smaller by about 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'data': 0,\n",
       " u'does': 1,\n",
       " u'don': 2,\n",
       " u'edu': 3,\n",
       " u'god': 4,\n",
       " u'good': 5,\n",
       " u'graphics': 6,\n",
       " u'image': 7,\n",
       " u'jesus': 8,\n",
       " u'just': 9,\n",
       " u'know': 10,\n",
       " u'like': 11,\n",
       " u'nasa': 12,\n",
       " u'people': 13,\n",
       " u'say': 14,\n",
       " u'space': 15,\n",
       " u'think': 16,\n",
       " u'time': 17,\n",
       " u'use': 18,\n",
       " u'way': 19}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what are the 20 words that are most common in the whole corpus?\n",
    "most20common = CountVectorizer(max_features=20, stop_words='english')\n",
    "most20common.fit_transform(data_train.data)\n",
    "most20common.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi,\\n\\nI've noticed that if you only save a mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nSeems to be, barring evidence to the contr...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n &gt;In article &lt;1993Apr19.020359.26996@sq.sq.c...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have a request for those who would like to s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AW&amp;ST  had a brief blurb on a Manned Lunar Exp...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  class\n",
       "0  Hi,\\n\\nI've noticed that if you only save a mo...      1\n",
       "1  \\n\\nSeems to be, barring evidence to the contr...      3\n",
       "2  \\n >In article <1993Apr19.020359.26996@sq.sq.c...      2\n",
       "3  I have a request for those who would like to s...      0\n",
       "4  AW&ST  had a brief blurb on a Manned Lunar Exp...      2"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what are the 20 most common words in each of the 4 classes?\n",
    "X = pd.DataFrame(data_train.data, columns =['data'])\n",
    "y = pd.DataFrame(data_train.target, columns =['class'])\n",
    "final = pd.concat([X, y], axis = 1)\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [u'people', u'time', u'argument', u'say', u'religion', u'atheists', u'don', u'jesus', u'does', u'way', u'true', u'atheism', u'said', u'just', u'think', u'bible', u'like', u'god', u'believe', u'know']\n",
      "1 [u'format', u'data', u'image', u'gif', u'ftp', u'graphics', u'does', u'software', u'available', u'use', u'pub', u'like', u'images', u'file', u'edu', u'jpeg', u'color', u'program', u'files', u'know']\n",
      "2 [u'people', u'time', u'data', u'just', u'year', u'space', u'launch', u'orbit', u'new', u'don', u'lunar', u'shuttle', u'like', u'earth', u'satellite', u'moon', u'program', u'mission', u'nasa', u'use']\n",
      "3 [u'life', u'people', u'time', u'say', u'jesus', u'does', u'way', u'think', u'don', u'said', u'just', u'did', u'bible', u'good', u'believe', u'point', u'like', u'god', u'christian', u'know']\n"
     ]
    }
   ],
   "source": [
    "li = []\n",
    "for i in range(4):\n",
    "    mc420 = CountVectorizer(max_features=20, stop_words='english')\n",
    "    mc420.fit_transform(final[final['class'] == i]['data'])\n",
    "    print i, [x for x in mc420.vocabulary_]\n",
    "    li.append([x for x in mc420.vocabulary_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.745011086475\n",
      "con mat: \n",
      "[[187  16  46  70]\n",
      " [ 13 345  28   3]\n",
      " [ 22  23 333  16]\n",
      " [ 67  14  27 143]]\n",
      "number of words: 26576\n"
     ]
    }
   ],
   "source": [
    "# evaluate the performance of a Logistic Regression on the features extracted by the CountVectorizer\n",
    "# you will have to transform the test_set too. Be carefule to use the trained vectorizer, without re-fitting it\n",
    "def score(v):\n",
    "    X = v.fit_transform(data_train.data)\n",
    "    y = v.transform(data_test.data)\n",
    "\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X, data_train.target)\n",
    "    y_pred = lr.predict(y)\n",
    "    print 'score:', lr.score(y, data_test.target)\n",
    "    print 'con mat: \\n', confusion_matrix(data_test.target, y_pred)\n",
    "    print 'number of words:', len(v.vocabulary_)\n",
    "    \n",
    "v = CountVectorizer(stop_words='english')\n",
    "score(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "significantly worse with limited features\n",
      "score: 0.49519586105\n",
      "con mat: \n",
      "[[159  98  30  32]\n",
      " [ 30 296  55   8]\n",
      " [ 47 147 186  14]\n",
      " [112  91  19  29]]\n",
      "number of words: 20\n"
     ]
    }
   ],
   "source": [
    "# try the following 3 modification:\n",
    "# restrict the max_features\n",
    "\n",
    "print \"significantly worse with limited features\"\n",
    "v = CountVectorizer(stop_words='english', max_features=20)\n",
    "score(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.441241685144\n",
      "con mat: \n",
      "[[116 125  41  37]\n",
      " [ 30 287  67   5]\n",
      " [ 39 181 170   4]\n",
      " [ 88 112  27  24]]\n",
      "number of words: 18\n"
     ]
    }
   ],
   "source": [
    "# change max_df and min_df\n",
    "v = CountVectorizer(stop_words='english', min_df=0.1,max_df=0.9)\n",
    "score(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use a fixed vocabulary of size 80 combining the 20 most common words per group found earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = CountVectorizer(li, stop_words='english')\n",
    "score(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hashing and TF-IDF\n",
    "\n",
    "Let's see if Hashing or TF-IDF improves the accuracy.\n",
    "\n",
    "1. Initialize a HashingVectorizer and repeat the test with no restriction on the number of features\n",
    "- does the score improve with respect to the count vectorizer?\n",
    "    - can you change any of the default parameters to improve it?\n",
    "- print out the number of features for this model\n",
    "- Initialize a TF-IDF Vectorizer and repeat the analysis above\n",
    "- can you improve on your best score above?\n",
    "    - can you change any of the default parameters to improve it?\n",
    "- print out the number of features for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.674057649667\n",
      "con mat: \n",
      "[[180  41  54  44]\n",
      " [ 15 337  34   3]\n",
      " [ 29  47 315   3]\n",
      " [100  29  42  80]]\n",
      "number of features: 1048576\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hv = HashingVectorizer()\n",
    "X = hv.fit_transform(data_train.data)\n",
    "y = hv.transform(data_test.data)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X, data_train.target)\n",
    "y_pred = lr.predict(y)\n",
    "print 'score:', lr.score(y, data_test.target)\n",
    "print 'con mat: \\n', confusion_matrix(data_test.target, y_pred)\n",
    "print 'number of features:', v.n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score is slightly worse than the CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.747967479675\n",
      "con mat: \n",
      "[[198  15  65  41]\n",
      " [  8 351  29   1]\n",
      " [ 17  21 356   0]\n",
      " [ 82  16  46 107]]\n",
      "number of words: 26576\n"
     ]
    }
   ],
   "source": [
    "# Initialize a TF-IDF Vectorizer and repeat the analysis above\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(stop_words='english')\n",
    "X = tv.fit_transform(data_train.data)\n",
    "y = tv.transform(data_test.data)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X, data_train.target)\n",
    "y_pred = lr.predict(y)\n",
    "print 'score:', lr.score(y, data_test.target)\n",
    "print 'con mat: \\n', confusion_matrix(data_test.target, y_pred)\n",
    "print 'number of words:', len(tv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classifier comparison\n",
    "\n",
    "Of all the vectorizers tested above, choose one that has a reasonable performance with a manageable number of features and compare the performance of these models:\n",
    "\n",
    "- KNN\n",
    "- Logistic Regression\n",
    "- Decision Trees\n",
    "- Support Vector Machine\n",
    "- Random Forest\n",
    "- Extra Trees\n",
    "\n",
    "In order to speed up the calculation it's better to vectorize the data only once and then compare the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "score: 0.339246119734\n",
      "con mat: \n",
      "[[199  33  21  66]\n",
      " [184 119  20  66]\n",
      " [191  45  74  84]\n",
      " [148  22  14  67]]\n",
      "number of words: 26576\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "score: 0.745011086475\n",
      "con mat: \n",
      "[[187  16  46  70]\n",
      " [ 13 345  28   3]\n",
      " [ 22  23 333  16]\n",
      " [ 67  14  27 143]]\n",
      "number of words: 26576\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "score: 0.610495195861\n",
      "con mat: \n",
      "[[149  49  40  81]\n",
      " [ 20 318  46   5]\n",
      " [ 36  80 263  15]\n",
      " [ 94  38  23  96]]\n",
      "number of words: 26576\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "score: 0.305247597931\n",
      "con mat: \n",
      "[[  3 314   2   0]\n",
      " [  0 389   0   0]\n",
      " [  0 373  21   0]\n",
      " [  0 249   2   0]]\n",
      "number of words: 26576\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "score: 0.646711012565\n",
      "con mat: \n",
      "[[181  31  47  60]\n",
      " [ 23 328  30   8]\n",
      " [ 54  67 264   9]\n",
      " [ 94  26  29 102]]\n",
      "number of words: 26576\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False)\n",
      "score: 0.643754619364\n",
      "con mat: \n",
      "[[193  39  40  47]\n",
      " [ 21 339  25   4]\n",
      " [ 47  74 261  12]\n",
      " [102  34  37  78]]\n",
      "number of words: 26576\n"
     ]
    }
   ],
   "source": [
    "models = [KNeighborsClassifier(), LogisticRegression(), DecisionTreeClassifier(),\n",
    "          SVC(), RandomForestClassifier(), ExtraTreesClassifier()]\n",
    "\n",
    "def tester(model, v):\n",
    "    X = v.fit_transform(data_train.data)\n",
    "    y = v.transform(data_test.data)\n",
    "    print model\n",
    "    mod = model\n",
    "    mod.fit(X, data_train.target)\n",
    "    y_pred = mod.predict(y)\n",
    "    print 'score:', mod.score(y, data_test.target)\n",
    "    print  'con mat:','\\n', confusion_matrix(data_test.target, y_pred)\n",
    "    print 'number of words:', len(v.vocabulary_)\n",
    "v = CountVectorizer(stop_words='english', max_df=0.5)\n",
    "for i in models:\n",
    "    tester(i, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Other classifiers\n",
    "\n",
    "Adapt the code from [this example](http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html#example-text-document-classification-20newsgroups-py) to compare across all the classifiers suggested and to display the final plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: NLTK\n",
    "\n",
    "NLTK is a vast library. Can you find some interesting bits to share with classmates?\n",
    "Start here: http://www.nltk.org/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
